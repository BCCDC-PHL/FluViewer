import os
import json
import logging

from pathlib import Path

log = logging.getLogger(__name__)


def _convert_cell_to_int(cell: str) -> int:
    """
    Cells are reported with a "K", "M", or "G" suffix in bbnorm log.
    This function converts the cell size to an integer.

    :param cell: The cell size.
    :type cell: str
    :return: The cell size as an integer.
    :rtype: int
    """
    cell_suffix = cell[-1]
    cell_size = float(cell[:-1])
    if cell_suffix == "K":
        cell_size = cell_size * 1000
    elif cell_suffix == "M":
        cell_size = cell_size * 1000000
    elif cell_suffix == "B":
        cell_size = cell_size * 1000000000

    cell_size_int = round(cell_size)

    return cell_size_int


def parse_bbnorm_log(bbnorm_log_path: Path) -> dict:
    """
    Parse the log file generated by BBnorm.

    :param bbnorm_log_path: The path to the log file generated by BBnorm.
    :type bbnorm_log_path: str
    :return: A dictionary with the parsed log file.
    :rtype: dict
    """
    int_fields = [
        'threads',
        'k',
        'passes',
        'bits_per_cell',
        'hashes',
        'base_min_quality',
        'target_depth',
        'min_depth',
        'max_depth',
        'min_good_kmers',
        'estimated_unique_kmers',
        'total_kmers_counted',
        'total_unique_kmer_count',
    ]
    float_fields = [
        'kmer_min_prob',
        'depth_percentile',
        'corrected_depth_average',
        'approx_read_depth_median',
    ]
    bool_fields = [
        'deterministic',
        'toss_error_reads',
        'ignore_dupe_kmers',
        'fix_spikes',
    ]
    percent_fields = [
        'percent_unique',
    ]
    value_with_percent_fields = [
        'total_reads_in',
        'total_bases_in',
        'error_reads_in',
        'error_pairs_in',
        'error_type_1',
        'error_type_2',
        'error_type_3',
    ]
    seconds_fields = [
        'table_creatiion_time',
    ]
    seconds_with_rate_fields = [
        'table_read_time',
    ]

    unique_or_all_kmers_fields = [
        'depth_average',
        'depth_median',
        'depth_standard_deviation',
        'corrected_depth_average',
    ]
    pass_num_key = 'pass_0'
    parsed_bbnorm_log = {
        pass_num_key: {}
    }
    with open(bbnorm_log_path, 'r') as f:
        for line in f:
            bbnorm_key = None
            bbnorm_value = None
            line = line.strip()
            if "Pass" in line:
                line_split = line.split()
                try:
                    pass_num = int(line_split[2])
                except ValueError as e:
                    log.error(f"Could not parse pass number number from bbnrorm log line: {line}")
                    raise e
                pass_num_key = 'pass_' + str(pass_num)
                parsed_bbnorm_log[pass_num_key] = {}

            if line.startswith("Settings:"):
                continue
            if line.startswith("Started output threads."):
                continue
            if line.startswith("Removing temp files."):
                continue
            if line.startswith("Total time:"):
                time_str = line.split(":")[1].strip()
                time_str_split = time_str.split()
                time_seconds = float(time_str_split[0])
                time_units = time_str_split[1]
                parsed_bbnorm_log['total_time_seconds'] = time_seconds
                rate_str = time_str_split[2]
                rate = float(rate_str)
                rate_units = time_str_split[3]
                rate_units = rate_units.replace("/", "_per_")
                parsed_bbnorm_log['total_rate_' + rate_units] = rate
                
                continue

            if not line.startswith("Made hash table:"):
                line_split = line.split(":")
                if len(line_split) != 2:
                    continue
                
                bbnorm_key = line_split[0].lower().replace(" ", "_")
                
                bbnorm_value_str = line_split[1].strip()
                if bbnorm_key in int_fields:
                    try:
                        bbnorm_value = int(bbnorm_value_str)
                    except ValueError as e:
                        log.error(f"Could not parse integer value from bbnorm log line: {line}")
                        bborm_value = None
                elif bbnorm_key in float_fields:
                    try:
                        bbnorm_value = float(bbnorm_value_str)
                    except ValueError as e:
                        log.error(f"Could not parse float value from bbnorm log line: {line}")
                        bbnorm_value = None
                elif bbnorm_key in bool_fields:
                    try:
                        bbnorm_value = bool(bbnorm_value_str)
                    except ValueError as e:
                        log.error(f"Could not parse boolean value from bbnorm log line: {line}")
                        bbnorm_value = None
                elif bbnorm_key in percent_fields:
                    bbnorm_value = float(bbnorm_value_str.split("%")[0])
                    parsed_bbnorm_log[pass_num_key][bbnorm_key] = bbnorm_value
                elif bbnorm_key == "cells":
                    bbnorm_value = _convert_cell_to_int(bbnorm_value_str)
                elif bbnorm_key == "table_creation_time":
                    bbnorm_key = "table_creation_time_seconds"
                    bbnorm_value = float(bbnorm_value_str.split(" ")[0])
                elif bbnorm_key in value_with_percent_fields:
                    bbnorm_value_int_str = bbnorm_value_str.split(" ")[0]
                    bbnorm_value_int = int(bbnorm_value_int_str)
                    parsed_bbnorm_log[pass_num_key][bbnorm_key] = bbnorm_value_int
                    bbnorm_value_percent_str = bbnorm_value_str.split()[1]
                    bbnorm_value_percent = float(bbnorm_value_percent_str.split("%")[0])
                    if len(bbnorm_value_str.split()) == 3 and bbnorm_key.endswith("_in"):
                        bbnorm_key = bbnorm_key.strip("_in") + "_kept_percent"
                        parsed_bbnorm_log[pass_num_key][bbnorm_key] = bbnorm_value_percent
                    else:
                        bbnorm_key = bbnorm_key + "_percent"
                        parsed_bbnorm_log[pass_num_key][bbnorm_key] = bbnorm_value_percent
                elif bbnorm_key == "approx._read_depth_median":
                    bbnorm_key = "approx_read_depth_median"
                    bbnorm_value = float(bbnorm_value_str)
                    parsed_bbnorm_log[pass_num_key][bbnorm_key] = bbnorm_value
                elif bbnorm_key in seconds_fields:
                    bbnorm_seconds_float_str = bbnorm_value_str.split(" ")[0]
                    bbnorm_seconds = float(bbnorm_seconds_float_str)
                    parsed_bbnorm_log[pass_num_key][bbnorm_key + "_seconds"] = bbnorm_seconds
                elif bbnorm_key in seconds_with_rate_fields:
                    bbnorm_seconds_float_str = bbnorm_value_str.split(" ")[0]
                    bbnorm_seconds = float(bbnorm_seconds_float_str)
                    parsed_bbnorm_log[pass_num_key][bbnorm_key + "_seconds"] = bbnorm_seconds
                    
                    bbnorm_rate_str = bbnorm_value_str.split()[2]
                    bbnorm_rate = float(bbnorm_rate_str)

                    rate_units = bbnorm_value_str.split()[3]
                    rate_units = rate_units.replace("/", "_per_")
                    bbnorm_key = bbnorm_key + "_" + rate_units
                    parsed_bbnorm_log[pass_num_key][bbnorm_key] = bbnorm_rate
                elif bbnorm_key in unique_or_all_kmers_fields:
                    bbnorm_value_str_split = bbnorm_value_str.split()
                    bbnorm_value = float(bbnorm_value_str_split[0])
                    unique_or_all_kmers = "_".join([
                        bbnorm_value_str_split[1].replace("(", ""),
                        bbnorm_value_str_split[2].replace(")", "")
                    ])
                    bbnorm_key = bbnorm_key + "_" + unique_or_all_kmers
                    parsed_bbnorm_log[pass_num_key][bbnorm_key] = bbnorm_value
                    
                else:
                    bbnorm_value = bbnorm_value_str
                    parsed_bbnorm_log[pass_num_key][bbnorm_key] = bbnorm_value

            elif line.startswith("Made hash table:"):
                hash_table = {}
                line_split = line.split(":")
                bbnorm_key = line_split[0].lower().replace(" ", "_")
                try:
                    hash_table_str = line_split[1].strip()
                    # split on multiple spaces
                    hash_table_str_split = hash_table_str.split("  ")
                    for hash_table_metric_str in hash_table_str_split:
                        hash_table_metric_split = hash_table_metric_str.split("=")
                        hash_table_metric_key = hash_table_metric_split[0].strip().lower().replace(" ", "_")
                        hash_table_metric_value_str = hash_table_metric_split[1].strip()
                        if hash_table_metric_key == 'hashes':
                            hash_table_metric_value = int(hash_table_metric_value_str)
                        elif hash_table_metric_key == 'mem':
                            hash_table_metric_value = float(hash_table_metric_value_str.split(" ")[0])
                            hash_table_metric_units = hash_table_metric_value_str.split(" ")[1]
                            hash_table_metric_key = hash_table_metric_key + "_" + hash_table_metric_units
                        elif hash_table_metric_key == 'cells':
                            hash_table_metric_value = _convert_cell_to_int(hash_table_metric_value_str)
                        elif hash_table_metric_key == 'used':
                            hash_table_metric_key = 'percent_used_memory'
                            hash_table_metric_value = float(hash_table_metric_value_str.split("%")[0])
                        else:
                            hash_table_metric_value = hash_table_metric_value_str

                        hash_table[hash_table_metric_key] = hash_table_metric_value
                        
                except IndexError as e:
                    log.error(f"Could not parse hash table from bbnorm log line: {line}")
                    raise e
                parsed_bbnorm_log[pass_num_key]['hash_table'] = hash_table
                
    parsed_bbnorm_log.pop('pass_0')

    return parsed_bbnorm_log


def _find_starts_ends_of_matches(alignment: str) -> list[dict]:
    """
    Find the start and end positions of matches in the alignment sequence.

    :param alignment: The alignment sequence, including gaps filled with '-' or 'N'
    :return: A list of dictionaries with keys 'start' and 'end' representing the start and end positions of matches.
    """
    starts_ends = []
    start = None
    end = None
    for i, char in enumerate(alignment):
        if char in 'ACTG':
            if start is None:
                start = i
            end = i
        else:
            if start is not None:
                starts_ends.append({'start': start, 'end': end})
                start = None
                end = None

    if start is not None:
        starts_ends.append({'start': start, 'end': end})

    return starts_ends


def parse_contig_alignment(alignment_path: Path) -> list[dict]:
    """
    Parse the alignment file and return a list of dictionaries representing the alignment of contigs to the scaffold.

    :param alignment_path: The path to the alignment file.
    :type alignment_path: Path
    :return: A list of dictionaries representing the alignment of contigs to the scaffold. Keys are 'id', 'seq', 'matches'.
    :rtype: list[dict]
    """
    alignment = []
    log.debug(f"Reading alignment file: {alignment_path}")
    with open(alignment_path, 'r') as f:
        contig = None
        contig_id = None
        for line in f:
            if line.startswith('>'):
                contig_id = line.strip().lstrip('>').split()[0]
                if contig is not None:
                    alignment.append(contig)
                
                contig = {'id': contig_id,
                          'seq': ''}
            else:
                contig['seq'] += line.strip()

        if contig is not None:
            alignment.append(contig)

    for contig in alignment:
        contig['matching_regions'] = []
        for match in _find_starts_ends_of_matches(contig['seq']):
            contig['matching_regions'].append(match)
            
    return alignment


def parse_scaffolds(scaffolds_path: Path, segment_to_parse: str) -> dict:
    """
    Parse the scaffolds file and return a dictionary of scaffold_id -> scaffold_sequence.

    :param scaffolds_path: The path to the scaffolds file.
    :type scaffolds_path: Path
    :param segment: The segment to parse (e.g. 'HA').
    :type segment: str
    :return: A dictionary with the parsed scaffold sequence. Keys are 'id', 'segment', 'seq' and 'matches'.
    :rtype: dict
    """
    segment_scaffold = {}
    with open(scaffolds_path, 'r') as f:
        scaffold_id = None
        scaffold_segment = None
        scaffold_seq = None
        segment_found = False
        for line in f:
            if line.startswith('>') and not segment_found:
                scaffold_id = line.strip().lstrip('>').split()[0]
                scaffold_segment = scaffold_id.split('|')[1].split('_')[0]
                if scaffold_segment == segment_to_parse:
                    segment_found = True
                    segment_scaffold['id'] = scaffold_id
                    segment_scaffold['segment'] = scaffold_segment
                    segment_scaffold['seq'] = ''
            elif segment_found and not line.startswith('>'):
                segment_scaffold['seq'] += line.strip()

            elif segment_found and line.startswith('>'):
                segment_found = False

    segment_scaffold['matching_regions'] = []
    for match in _find_starts_ends_of_matches(segment_scaffold['seq']):
        segment_scaffold['matching_regions'].append(match)
        
    return segment_scaffold
